{
    "abstract": "A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (\u201csemantic structure\u201d) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 or-thogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are re-turned. initial tests find this completely automatic method for retrieval to be promising.",
    "author": [
        "Scott C. Deerwester",
        "Susan T. Dumais",
        "Thomas K. Landauer",
        "George W. Furnas",
        "Richard A. Harshman"
    ],
    "citation_contexts": [
        null,
        "lassification analyses are frequently used for term and document clustering (Sparck Jones, 1971; Salton, 1968; Jardin & van Rijsbergen, 1971). Latent class analysis (Baker, 1962) and factor analysis (=-=Atherton & Borko, 1965-=-; Borko & Bemick, 1963; Ossorio, 1966) have also been explored before for automatic document indexing and retrieval. In document clustering, for example, a notion of distance is defined such that two  ess. The explicit representation of both terms and documents in the same space makes retrieving documents relevant to user queries a straightforward matter. Previous work by Borko and his colleagues (=-=Atherton & Borko, 1965-=-; Borko & Bemick, 1963) is similar in name to our approach, but used the factor space only for document clustering, not document retrieval, and computational simplifications reduced its representation",
        "in the literature. Hierarchical classification analyses are frequently used for term and document clustering (Sparck Jones, 1971; Salton, 1968; Jardin & van Rijsbergen, 1971). Latent class analysis (=-=Baker, 1962-=-) and factor analysis (Atherton & Borko, 1965; Borko & Bemick, 1963; Ossorio, 1966) have also been explored before for automatic document indexing and retrieval. In document clustering, for example, a",
        "en reported in studies of interindexer consistency (Tarr & Borko, 1974) and in the generation of search terms by either expert intermediaries (Fidel, 1985) or less experienced searchers (Liley, 1954; =-=Bates, 1986-=-). The prevalence of synonyms tends to decrease the \u201crecall\u201d performance of retrieval systems. By polysemy we refer to the general fact that most words have more than one distinct meaning (homography)",
        "re frequently used for term and document clustering (Sparck Jones, 1971; Salton, 1968; Jardin & van Rijsbergen, 1971). Latent class analysis (Baker, 1962) and factor analysis (Atherton & Borko, 1965; =-=Borko & Bemick, 1963-=-; Ossorio, 1966) have also been explored before for automatic document indexing and retrieval. In document clustering, for example, a notion of distance is defined such that two documents are consider entation of both terms and documents in the same space makes retrieving documents relevant to user queries a straightforward matter. Previous work by Borko and his colleagues (Atherton & Borko, 1965; =-=Borko & Bemick, 1963-=-) is similar in name to our approach, but used the factor space only for document clustering, not document retrieval, and computational simplifications reduced its representational power. In Borko and",
        "have positions in the structure. Then a query can be placed at the centroid of its term points. Thus for both elegance and retrieval mechanisms, we needed what are called two-mode proximity methods (=-=Carroll and Arabie, 1980-=-), that start with a rectangular matrix and construct explicit representations of both row and column objects. One such method is multidimensional unfolding (Coombs, 1964; Heiser, 1981; Desarbo & Carr ther in some space or structure. Such models include: hierarchical, partition and overlapping clusterings; ultrametric and additive trees; and factoranalytic and multidimensional distance models (see =-=Carroll & Arabie, 1980-=- for a survey). Aiding information retrieval by discovering latent proximity structure has at least two lines of precedence in the literature. Hierarchical classification analyses are frequently used",
        "terms and documents would appear as points in a single space with similarity related monotonically to Euclidean distance. Another is two-mode factor analysis (Harshman, 1970; Harshman & Lundy, 1984a; =-=Carroll & Chang, 1970-=-; Kruskal, 1978), in which terms and documents would again be represented as points in a space, but similarity is given by the inner product between points. A final candidate is unfolding in trees (Fu",
        null,
        "roximity methods (Carroll and Arabie, 1980), that start with a rectangular matrix and construct explicit representations of both row and column objects. One such method is multidimensional unfolding (=-=Coombs, 1964-=-; Heiser, 1981; Desarbo & Carroll. 1985), in which both terms and documents would appear as points in a single space with similarity related monotonically to Euclidean distance. Another is two-mode fa",
        "ative numerical solution of multi-mode factor-analysis problems, was used for the studies reported below. (Other programs for more standard SVD are also available-e.g., Golub, Luk, and Overton, 1981; =-=Cullum, Willoughby, and Lake, 1983-=-.) \u201cDocuments\u201d consist of the full text of the title and abstract. Each document is indexed automatically; all terms occurring in more than one document and not on a stop list of 439 common words used",
        null,
        "Gomez, & Dumais, 1987). Comparably poor agreement has been reported in studies of interindexer consistency (Tarr & Borko, 1974) and in the generation of search terms by either expert intermediaries (=-=Fidel, 1985-=-) or less experienced searchers (Liley, 1954; Bates, 1986). The prevalence of synonyms tends to decrease the \u201crecall\u201d performance of retrieval systems. By polysemy we refer to the general fact that mo",
        null,
        null,
        "in recall rate without overall loss of precision as more indexing terms, either taken from the documents or from large samples of actual users\u2019 words are added (Gomez, Lochbaum, & Landauer, in press; =-=Fumas, 1985-=-). Whether this \u201cunlimited aliasing\u201d method, which we have described elsewhere, will be effective in very large data bases reTABLE 1. Sample term by document matrix.\u201d mains to be determined. Not only",
        null,
        null,
        "Desarbo & Carroll. 1985), in which both terms and documents would appear as points in a single space with similarity related monotonically to Euclidean distance. Another is two-mode factor analysis (=-=Harshman, 1970-=-; Harshman & Lundy, 1984a; Carroll & Chang, 1970; Kruskal, 1978), in which terms and documents would again be represented as points in a space, but similarity is given by the inner product between poi",
        "ll. 1985), in which both terms and documents would appear as points in a single space with similarity related monotonically to Euclidean distance. Another is two-mode factor analysis (Harshman, 1970; =-=Harshman & Lundy, 1984-=-a; Carroll & Chang, 1970; Kruskal, 1978), in which terms and documents would again be represented as points in a space, but similarity is given by the inner product between points. A final candidate i nts both terms and documents as vectors in a space of choosable dimensionality, and the dot product or cosine between points in the space gives their similarity. In addition, a program was available (=-=Harshman & Lundy, 1984-=-b) that fit the model in time of order N2 X k\u2019. SVD or Two-Mode Factor Analysis Overview The latent semantic structure analysis starts with a matrix of terms by documents. This matrix is then analyzed of the SVD Latent Semantic Indexing (LSI) Method We have so far tried the LSI method on two standard document collections where queries and relevance judgments were available (MED and CISI). PARAFAC (=-=Harshman & Lundy, 1984-=-b), a program for the iterative numerical solution of multi-mode factor-analysis problems, was used for the studies reported below. (Other programs for more standard SVD are also available-e.g., Golub",
        "ll. 1985), in which both terms and documents would appear as points in a single space with similarity related monotonically to Euclidean distance. Another is two-mode factor analysis (Harshman, 1970; =-=Harshman & Lundy, 1984-=-a; Carroll & Chang, 1970; Kruskal, 1978), in which terms and documents would again be represented as points in a space, but similarity is given by the inner product between points. A final candidate i nts both terms and documents as vectors in a space of choosable dimensionality, and the dot product or cosine between points in the space gives their similarity. In addition, a program was available (=-=Harshman & Lundy, 1984-=-b) that fit the model in time of order N2 X k\u2019. SVD or Two-Mode Factor Analysis Overview The latent semantic structure analysis starts with a matrix of terms by documents. This matrix is then analyzed of the SVD Latent Semantic Indexing (LSI) Method We have so far tried the LSI method on two standard document collections where queries and relevance judgments were available (MED and CISI). PARAFAC (=-=Harshman & Lundy, 1984-=-b), a program for the iterative numerical solution of multi-mode factor-analysis problems, was used for the studies reported below. (Other programs for more standard SVD are also available-e.g., Golub",
        "ds (Carroll and Arabie, 1980), that start with a rectangular matrix and construct explicit representations of both row and column objects. One such method is multidimensional unfolding (Coombs, 1964; =-=Heiser, 1981-=-; Desarbo & Carroll. 1985), in which both terms and documents would appear as points in a single space with similarity related monotonically to Euclidean distance. Another is two-mode factor analysis",
        null,
        null,
        null,
        "ld appear as points in a single space with similarity related monotonically to Euclidean distance. Another is two-mode factor analysis (Harshman, 1970; Harshman & Lundy, 1984a; Carroll & Chang, 1970; =-=Kruskal, 1978-=-), in which terms and documents would again be represented as points in a space, but similarity is given by the inner product between points. A final candidate is unfolding in trees (Fumas, 1980), in",
        null,
        "eement has been reported in studies of interindexer consistency (Tarr & Borko, 1974) and in the generation of search terms by either expert intermediaries (Fidel, 1985) or less experienced searchers (=-=Liley, 1954-=-; Bates, 1986). The prevalence of synonyms tends to decrease the \u201crecall\u201d performance of retrieval systems. By polysemy we refer to the general fact that most words have more than one distinct meaning",
        "term and document clustering (Sparck Jones, 1971; Salton, 1968; Jardin & van Rijsbergen, 1971). Latent class analysis (Baker, 1962) and factor analysis (Atherton & Borko, 1965; Borko & Bemick, 1963; =-=Ossorio, 1966-=-) have also been explored before for automatic document indexing and retrieval. In document clustering, for example, a notion of distance is defined such that two documents are considered close to the inary clustering (Borko & Bernick, 1963). Third, some attempts have relied on excessively tedious data gathering techniques, requiring the collection of thousands of similarity judgments from humans (=-=Ossorio, 1966-=-). Previously reported clustering and factor analytic approaches have also struggled with a certain representational awkwardness. Typically the original data explicitly relate two types of entities, t",
        null,
        "ering latent proximity structure has at least two lines of precedence in the literature. Hierarchical classification analyses are frequently used for term and document clustering (Sparck Jones, 1971; =-=Salton, 1968-=-; Jardin & van Rijsbergen, 1971). Latent class analysis (Baker, 1962) and factor analysis (Atherton & Borko, 1965; Borko & Bemick, 1963; Ossorio, 1966) have also been explored before for automatic doc",
        "ally only n parameters for n objects). Empirically, clustering improves the computational efficiency of search; whether or not it improves retrieval success is unclear (Jardin & van Rijsbergen, 1971; =-=Salton & McGill, 1983-=-; Voorhees, 1985). JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE-September 1990 393sPreviously tried factor analytic approaches have taken a square symmetric matrix of similarities between p",
        null,
        null,
        "e main key word for a single well-known object less than 20% of the time (Furnas, Landauer, Gomez, & Dumais, 1987). Comparably poor agreement has been reported in studies of interindexer consistency (=-=Tarr & Borko, 1974-=-) and in the generation of search terms by either expert intermediaries (Fidel, 1985) or less experienced searchers (Liley, 1954; Bates, 1986). The prevalence of synonyms tends to decrease the \u201crecall",
        null,
        "for n objects). Empirically, clustering improves the computational efficiency of search; whether or not it improves retrieval success is unclear (Jardin & van Rijsbergen, 1971; Salton & McGill, 1983; =-=Voorhees, 1985-=-). JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE-September 1990 393sPreviously tried factor analytic approaches have taken a square symmetric matrix of similarities between pairs of document"
    ],
    "cited_paper_doi": [
        "684004",
        "156400",
        "156374",
        "10220901",
        "156363",
        "66386",
        "684005",
        "562925",
        "226487",
        "622727",
        "95504",
        "60487",
        "274434",
        "1167383",
        "1167382",
        "983103",
        "1167396",
        "1441",
        "477741",
        "1167392",
        "246720",
        "187831",
        "1167390",
        "1167385",
        "240637",
        "1108",
        "270232",
        "10220908",
        "2025077",
        "1167386",
        "1167389",
        "169550",
        "1167384",
        "273352",
        "855980",
        "254210",
        "10220909",
        "10220910",
        "10606070",
        "10606071",
        "10606072",
        "11222",
        "1167391"
    ],
    "cited_paper_url": [
        "http://citeseerx.ist.psu.edu/viewdoc/similar?doi=10.1.1.115.8343&type=sc",
        "http://citeseerx.ist.psu.edu/viewdoc/similar?doi=10.1.1.103.8364&type=sc"
    ],
    "doi": "10.1.1.108.8490",
    "title": "Indexing by Latent Semantic Analysis."
}
